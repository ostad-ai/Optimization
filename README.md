# Optimization
1) **Evolution Strategy (ES):** The basic algorithm in Python 
2) **Evolution Strategy (ES):** With single adaptive mutation step-size in Python
3) **Hill Climbing:** This is a *local search* algorithm that always tries to go up the hill of the fitness landspace. However, it gets stuck into local maxima. The Python code along an example is provided here.
4) **Simulated Annealing:** This is a metaheuristic to find hopefully the global optimum (minimum). It can be viewed as a combination of *Hill Climbing* and *random walk*. The code in python with two examples is given.
5) **Gradient (Steepest) Descent Method:** This method uses the negative of gradient of the given objective function as the direction to update the paramters of the function in order to get closer to its minimum. Two applications of the Gradient Descent method are provided here.
6) **Convex sets and convex functions:** Sets can be convex or concave. Functions can be convex, concave, or both, or neither. If function f is convex, then -f is concave. In this post, we examine conditions for functions to be convex. Convex functions are important for a large class of optimization problems. 
