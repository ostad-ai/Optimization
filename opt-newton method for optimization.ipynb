{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02ddd32e",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "### Newton's method\n",
    "In **Newton's method**, the inverse of **Hessian** matrix multiplied by the negative of **gradient** of the given objective function $F(\\boldsymbol{x})$ is used to update the current guess $\\boldsymbol{x_k}$:\n",
    "<br>$\\boldsymbol{x}_{k+1}=\\boldsymbol{x}_k-\\eta_k H_F(\\boldsymbol{x}_k)^{-1}\\nabla F(\\boldsymbol{x}_k)$\n",
    "<br> where $\\eta_k>0$ is the **learning rate** (also called *step size*) at time step $k$. \n",
    "<br>**Hint 1:** In the original formulation of Newton's method, $\\eta_k=1$ for all $k$.\n",
    "<br> **Hint 2:** If the initial guess $\\boldsymbol{x}_0$ is close enough to the minimum point and the Hessian is **nonsingular**, then the sequences $x_k$ converges to the minimizer. During these iterations, we must have a **positive definite** Hessian matrix to get to the minimizer.\n",
    "<br> **Reminder 1:** A square matrix is nonsingular if it has *inverse*. In fact, a matrix is nonsingular if and only if its *determinant* is nonzero.\n",
    "<br> **Reminder 2:** A *symmetric square* matrix, say $A$, is *positive definite* if and only if: \n",
    "- $\\boldsymbol{x}^TA\\;\\boldsymbol{x}>0 \\;\\; \\forall \\boldsymbol{x}\\in R^q-\\{0\\}$\n",
    "<br><br>**Contents**\n",
    "- The Python code of the Newton's method for optimization\n",
    "- Example with Booth function (convex function)\n",
    "- Example with Beale function (nonconvex function)\n",
    "    - With the initial guess near enough to the minimizer\n",
    "    - With the initial guess not close enough to the minimizer\n",
    "    \n",
    "<hr>\n",
    "The Python code at: https://github.com/ostad-ai/Optimization\n",
    "<br> Explanation: https://www.pinterest.com/HamedShahHosseini/Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0765f4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required modules\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "537ee639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function that we are going to optimize (minimize)\n",
    "# https://en.wikipedia.org/wiki/Test_functions_for_optimization\n",
    "# Booth is convex and has a minimum at f(1,3)=0\n",
    "def Booth(xs):\n",
    "    x,y=xs\n",
    "    f=(x+2*y-7)**2+(2*x+y-5)**2\n",
    "    return f\n",
    "\n",
    "# Beale is nonconvex and its global minimum is at f(3,0.5)=0\n",
    "def Beale(xs):\n",
    "    x,y=xs\n",
    "    f=(1.5-x+x*y)**2+(2.25-x+x*y**2)**2+(2.625-x+x*y**3)**2\n",
    "    return f\n",
    "\n",
    "# first order partial derivatives\n",
    "def partial(f,x0,i,h=.001):\n",
    "    x2=x0.copy(); x1=x0.copy()\n",
    "    x2[i]+=h; x1[i]-=h\n",
    "    diff=f(x2)-f(x1)\n",
    "    return diff/(2.*h)\n",
    "\n",
    "# second order partial derivatives\n",
    "def partial2(f,x0,i,j,h=.001):\n",
    "    x2=x0.copy(); x1=x0.copy()\n",
    "    x2[i]+=h; x1[i]-=h\n",
    "    f2=partial(f,x2,j,h)\n",
    "    f1=partial(f,x1,j,h)\n",
    "    return (f2-f1)/(2.*h)\n",
    "\n",
    "# returns gradient in a column vector\n",
    "def gradient(f,x,h=.001):\n",
    "    q=x.shape[0]\n",
    "    gradf=np.zeros(q)\n",
    "    for i in range(q):\n",
    "        gradf[i]=partial(f,x,i,h)\n",
    "    return gradf.reshape(-1,1)\n",
    "\n",
    "# returns the Hessian matrix of function f\n",
    "def hessian(f,x,h=.001,landa=.0):\n",
    "    q=x.shape[0]\n",
    "    hMat=np.zeros((q,q)) #Hessian matrix\n",
    "    for i in range(q):\n",
    "        for j in range(q):\n",
    "            hMat[i,j]=partial2(f,x,i,j,h)\n",
    "            if i==j: \n",
    "                # to make it positive definite\n",
    "                hMat[i,j]+=landa \n",
    "    return hMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1cd0b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the newton method for optimization\n",
    "def newtonOpt(x0,f,iter=100,etta=.05):\n",
    "    x=x0.copy().astype('float')\n",
    "    for i in range(iter):\n",
    "        hg=np.linalg.inv(hessian(f,x))@gradient(f,x)\n",
    "        x-=etta*hg\n",
    "    return x.flatten(),f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ede8e4a",
   "metadata": {},
   "source": [
    "Since the Booth function is **quadaric**, the Newton's method can get to the *minimizer* in just one iteration with $\\eta=1$. Let's try this in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d25cc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Newton's method for Booth function ---\n",
      "Initial guess=[0. 0.]\n",
      "Final solution=[1. 3.] \n",
      "with fitness=[3.88235686e-18]\n"
     ]
    }
   ],
   "source": [
    "# Example: finding minimum of Booth function in one iteration\n",
    "x0=np.array([0.,0.]).reshape(-1,1) # initial point or guess \n",
    "x_final,fitness=newtonOpt(x0,Booth,iter=1,etta=1)\n",
    "print('--- Newton\\'s method for Booth function ---')\n",
    "print(f'Initial guess={x0.flatten()}')\n",
    "print(f'Final solution={x_final} \\nwith fitness={fitness}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd54b5df",
   "metadata": {},
   "source": [
    "The Beale function is not convex, so if the initial guess is far from the optimium $x=[3,0.5]$, it gets stuck in a local minimum. In the following example, we choose a close enough initial guess for the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf459314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Newton's method for Beale function ---\n",
      "Initial guess=[2. 0.]\n",
      "Final solution=[2.97581249 0.49372117] \n",
      "with fitness=[9.72895881e-05]\n"
     ]
    }
   ],
   "source": [
    "# Example: finding minimum of Beale function\n",
    "# we choose a close enough initial guess to the true optimum [3,.5]\n",
    "x0=np.array([2.,0.]).reshape(-1,1) # initial point or guess \n",
    "x_final,fitness=newtonOpt(x0,Beale)\n",
    "print('--- Newton\\'s method for Beale function ---')\n",
    "print(f'Initial guess={x0.flatten()}')\n",
    "print(f'Final solution={x_final} \\nwith fitness={fitness}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ee253b",
   "metadata": {},
   "source": [
    "In the example below, for the **Beale** function, we start from an initial guess not close enough to the minimizer. \n",
    "So, it gets stuck in a **stationary point**, which is a **saddle point** here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7026d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Newton's method for Beale function ---\n",
      "Initial guess=[0. 0.]\n",
      "Final solution=[0.         0.99744639] \n",
      "with fitness=[14.203125]\n",
      "This time, it has converged to the saddle point (stationary point): [0,1]\n"
     ]
    }
   ],
   "source": [
    "# Example: finding minimum of Beale function\n",
    "# This time we choose a little far initial guess to the true optimum [3,.5]\n",
    "x0=np.array([0.,0.]).reshape(-1,1) # initial point or guess \n",
    "x_final,fitness=newtonOpt(x0,Beale)\n",
    "print('--- Newton\\'s method for Beale function ---')\n",
    "print(f'Initial guess={x0.flatten()}')\n",
    "print(f'Final solution={x_final} \\nwith fitness={fitness}')\n",
    "print('This time, it has converged to the saddle point (stationary point): [0,1]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adef068d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
