{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b68d094e",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "## Subgradient method\n",
    "A **subgradient** for a *convex* function $f:R^n\\rightarrow R$ at point $x_0$ is a vector $g\\in R^n$ such that:\n",
    "<br> $f(x)\\ge f(x_0)+g^T(x-x_0)$ for all $x\\in R^n$\n",
    "<br> The set of all subgradients of $f$ at point $x_0$ is called the **subdifferential** of $f$ at $x_0$, which is denoted by $\\partial f(x_0)$.\n",
    "<br> **Hint 1:** If $f$ is **differentiable**, then the subdifferential of $f$ at point $x_0$ will be $\\{\\nabla f(x_0)\\}$.\n",
    "<br>The **subgradient method** extends the *gradient descent* to convex non-differentiable functions:\n",
    "<br>\n",
    "<br> $x_0\\leftarrow$some initial guess\n",
    "<br> for $k$ in range(iter):\n",
    "<br>$\\;\\;x_{k+1}=x_k-\\eta_k g_k$\n",
    "<br>\n",
    "<br> where $g_k$ is any subgradient of $f$ at point $x_k$.\n",
    "<br>Here, $\\eta_k\\ge0$ is the **step size** (learning rate). It can be kept constant. Or you may decrease it with iteration $k$. Also, you may use fixed-length step size by $\\eta_k=\\frac{\\eta_0}{\\|g_k\\|_2}$ where $\\eta_0$ is a positive number.\n",
    "<br> **Hint 2:** We should keep the best solution found so far, since the subgradient method does not always choose a descent direction.\n",
    "<br>**Reminder:** In the examples here, we use the fixed-length step size for gradient method; because we find it more stable in the experiments.\n",
    "<br>The Python code at: https://github.com/ostad-ai/Optimization\n",
    "<br> Explanation: https://www.pinterest.com/HamedShahHosseini/Optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d00ceca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required modules\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dfc4be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimum solution is: [2/3,5] with objective value=0\n",
      "--------------------------------------------------\n",
      "The best solution by subgradient method: [0.66717988 5.00493542],\n",
      "which makes the objective function 0.01141048028687841\n"
     ]
    }
   ],
   "source": [
    "# the function f(x,y)=|3*x-2|+2*|y-5|, which we seek its minimum\n",
    "def func(z):\n",
    "    x,y=z.flatten()\n",
    "    return np.abs(3*x-2)+2*np.abs(y-5)\n",
    "\n",
    "# this function returns one subgradient of function defined above\n",
    "# in numpy module, numpy.sign(0) returns zero\n",
    "def subg_func(z):\n",
    "    x,y=z.flatten()\n",
    "    return np.array([3*np.sign(3*x-2),\n",
    "            2*np.sign(y-5)]).reshape(-1,1)\n",
    "\n",
    "iter=1000  # iteration \n",
    "etta0=.01  # initial step size\n",
    "x=np.array([1,1]).reshape(-1,1)  # initial guess\n",
    "# keep best solution found so far\n",
    "xbest=x.copy(); fbest=func(xbest)\n",
    "\n",
    "# subgradient loop\n",
    "for _ in range(iter):\n",
    "    gk=subg_func(x)\n",
    "    etta=etta0/np.sqrt(np.sum(gk**2))\n",
    "    x=x-etta*gk\n",
    "    fnow=func(x)\n",
    "    if fnow<fbest:\n",
    "        xbest=x\n",
    "        fbest=fnow\n",
    "        \n",
    "print('The optimum solution is: [2/3,5] with objective value=0')   \n",
    "print(50*'-')\n",
    "print(f'The best solution by subgradient method: {xbest.flatten()},'+\\\n",
    "      f'\\nwhich makes the objective function {fbest}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a3b68a",
   "metadata": {},
   "source": [
    "<hr>\n",
    "In the second example of this notebook, we want to use subgradient method for finding the minimum of the following function:\n",
    "<br> $|Ax-b|$\n",
    "<br>where $A$ is a matrix, and $b$ is a column vector. and $x$ is the unknown vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c36d732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of defining matrix A and vector b\n",
    "A=np.array([[1,2],[3,2]])\n",
    "b=np.array([6,4]).reshape(-1,1)\n",
    "# the exact solution is available by\n",
    "xstar=np.linalg.inv(A)@b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7937a34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimum solution is [-1.   3.5] by objective value=8.881784197001252e-16\n",
      "--------------------------------------------------\n",
      "The solution found by subgradient method is [-0.99391306  3.49608694]\n",
      "which provides the objective value=0.012173875177023064\n"
     ]
    }
   ],
   "source": [
    "# function |Ax-b|\n",
    "def func2(x,A,b):\n",
    "    return np.sum(np.abs(A@x-b))\n",
    "\n",
    "# one subgradient of |Ax-b|\n",
    "def subg_func2(x,A,b):\n",
    "    return A.T@np.sign(A@x-b)\n",
    "\n",
    "# solving |Ax-b| by subgradient method\n",
    "iter=1000 # iteration\n",
    "x=np.array([1,1]).reshape(-1,1) # initial guess\n",
    "etta0=.01  # step size parameter\n",
    "# best soluton found so far\n",
    "xbest=x.copy();fbest=func2(xbest,A,b)\n",
    "#subgradient loop\n",
    "for k in range(iter):\n",
    "    #etta=etta0/(k+1)\n",
    "    gk=subg_func2(x,A,b)\n",
    "    etta=etta0/np.sqrt(np.sum(gk**2))\n",
    "    x=x-etta*gk\n",
    "    fnow=func2(x,A,b)\n",
    "    if fnow<fbest:\n",
    "        xbest=x.copy(); fbest=fnow\n",
    "\n",
    "print(f'The optimum solution is {xstar.flatten()} by objective value={func2(xstar,A,b)}')\n",
    "print(50*'-')\n",
    "print(f'The solution found by subgradient method is {xbest.flatten()}'+\\\n",
    "      f'\\nwhich provides the objective value={fbest}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673819b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
